{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a9a9de1",
   "metadata": {},
   "source": [
    "# Processamento de Dados: Camada Raw para Silver\n",
    "**Disciplina:** Sistemas de Banco de Dados 2  \n",
    "**Semestre:** 2025/2  \n",
    "**Professor:** Thiago Luiz de Souza Gomes  \n",
    "**Grupo 15**\n",
    "\n",
    "**Integrantes:**\n",
    "* Caio Ferreira Duarte (231026901)\n",
    "* Laryssa Felix Ribeiro Lopes (231026840)\n",
    "* Luísa de Souza Ferreira (232014807)\n",
    "* Henrique Fontenelle Galvão Passos (231030771)\n",
    "* Marjorie Mitzi Cavalcante Rodrigues (231039140)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Contextualização e Objetivos\n",
    "Este notebook documenta a etapa de ETL (Extração, Transformação e Carga) do projeto. Na etapa anterior (\"Analytics\"), realizamos a exploração dos dados brutos de acidentes aéreos e identificamos diversas inconsistências, como problemas de codificação de texto, colunas com tipagem mista e valores nulos em métricas essenciais.\n",
    "\n",
    "O objetivo deste script é ler o arquivo CSV original, aplicar as regras de limpeza definidas no Dicionário de Dados e persistir o resultado na tabela `silver.aviao` no PostgreSQL. Esta tabela servirá como fonte confiável para a construção do modelo dimensional (Star Schema) na próxima etapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "4c8c9da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas importadas e variáveis de ambiente configuradas.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Ignoramos avisos de depreciação futura do Pandas para manter a saída do console limpa\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Configuração para visualizar todas as colunas no output do Jupyter\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Definição dos caminhos relativos dos arquivos\n",
    "# Optamos por usar caminhos relativos para que o código funcione em qualquer máquina do grupo\n",
    "ARQUIVO_RAW = '../Data_Layer/raw/dados_brutos.csv'\n",
    "ARQUIVO_DDL = '../Data_Layer/silver/ddl.sql'\n",
    "\n",
    "# String de conexão com o banco de dados (Container Docker)\n",
    "# Utiliza o driver psycopg2 via SQLAlchemy\n",
    "DB_URI = \"postgresql://admin:admin@localhost:5432/db_aviao\"\n",
    "\n",
    "print(\"Bibliotecas importadas e variáveis de ambiente configuradas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022e5561",
   "metadata": {},
   "source": [
    "## 2. Extração dos Dados (Camada Bronze)\n",
    "\n",
    "Durante a análise exploratória, notamos que o arquivo `dados_brutos.csv` não utiliza a codificação padrão UTF-8. Ao tentarmos abrir o arquivo convencionalmente, caracteres especiais eram corrompidos. Identificamos que a codificação correta é `cp1252` (Windows-1252).\n",
    "\n",
    "Abaixo, realizamos a leitura forçando esse encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "657919dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo arquivo: ../Data_Layer/raw/dados_brutos.csv\n",
      "Leitura concluída.\n",
      "Total de registros carregados: 88889\n"
     ]
    }
   ],
   "source": [
    "# Verificação de existência do arquivo antes da leitura\n",
    "if not os.path.exists(ARQUIVO_RAW):\n",
    "    print(f\"[ERRO] O arquivo não foi encontrado no caminho: {ARQUIVO_RAW}\")\n",
    "else:\n",
    "    print(f\"Lendo arquivo: {ARQUIVO_RAW}\")\n",
    "\n",
    "# Leitura do CSV\n",
    "try:\n",
    "    # low_memory=False é utilizado pois o Pandas identificou tipos mistos em algumas colunas\n",
    "    # durante a inferência inicial, o que consome mais memória mas garante a leitura correta.\n",
    "    df = pd.read_csv(ARQUIVO_RAW, encoding='cp1252', low_memory=False)\n",
    "    \n",
    "    print(\"Leitura concluída.\")\n",
    "    print(f\"Total de registros carregados: {df.shape[0]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Falha na leitura do CSV. Detalhes: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fd1b69",
   "metadata": {},
   "source": [
    "## 3. Padronização de Schema\n",
    "\n",
    "Para adequar os dados às boas práticas de banco de dados e ao padrão definido no nosso Dicionário de Dados, renomeamos as colunas originais (que usam pontos e CamelCase) para o padrão `snake_case` (minúsculas separadas por underline). Colunas irrelevantes para as perguntas de negócio do projeto foram descartadas nesta etapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "ba13a3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esquema de colunas padronizado.\n"
     ]
    }
   ],
   "source": [
    "# Dicionário de mapeamento (De: Nome Original -> Para: Nome no Banco)\n",
    "mapa_colunas = {\n",
    "    'Event.Id': 'event_id',\n",
    "    'Investigation.Type': 'investigation_type',\n",
    "    'Accident.Number': 'accident_number',\n",
    "    'Event.Date': 'event_date',\n",
    "    'Location': 'location',\n",
    "    'Country': 'country',\n",
    "    'Latitude': 'latitude',\n",
    "    'Longitude': 'longitude',\n",
    "    'Airport.Code': 'airport_code',\n",
    "    'Airport.Name': 'airport_name',\n",
    "    'Injury.Severity': 'injury_severity',\n",
    "    'Aircraft.damage': 'aircraft_damage',\n",
    "    'Aircraft.Category': 'aircraft_category',\n",
    "    'Registration.Number': 'registration_number',\n",
    "    'Make': 'make',\n",
    "    'Model': 'model',\n",
    "    'Amateur.Built': 'amateur_built',\n",
    "    'Number.of.Engines': 'number_of_engines',\n",
    "    'Engine.Type': 'engine_type',\n",
    "    'FAR.Description': 'far_description',\n",
    "    'Schedule': 'schedule',\n",
    "    'Purpose.of.flight': 'purpose_of_flight',\n",
    "    'Air.carrier': 'air_carrier',\n",
    "    'Total.Fatal.Injuries': 'total_fatal_injuries',\n",
    "    'Total.Serious.Injuries': 'total_serious_injuries',\n",
    "    'Total.Minor.Injuries': 'total_minor_injuries',\n",
    "    'Total.Uninjured': 'total_uninjured',\n",
    "    'Weather.Condition': 'weather_condition',\n",
    "    'Broad.phase.of.flight': 'broad_phase_of_flight',\n",
    "    'Report.Status': 'report_status',\n",
    "    'Publication.Date': 'publication_date'\n",
    "}\n",
    "\n",
    "# Seleciona apenas as colunas mapeadas e renomeia\n",
    "df = df[list(mapa_colunas.keys())].rename(columns=mapa_colunas)\n",
    "\n",
    "print(\"Esquema de colunas padronizado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f290f3",
   "metadata": {},
   "source": [
    "## 4. Tratamento e Limpeza de Dados\n",
    "\n",
    "Nesta etapa aplicamos as correções para os problemas de qualidade de dados identificados:\n",
    "\n",
    "1.  **Conversão de Datas:** As datas vieram como texto e em formatos mistos. Utilizamos a função `to_datetime` com tratamento de erros.\n",
    "2.  **Valores Nulos em Métricas:** Campos como `total_fatal_injuries` possuíam valores nulos (`NaN`). Para viabilizar somas e médias no SQL, assumimos que a ausência de informação indica zero feridos.\n",
    "3.  **Campo Severidade:** A coluna continha dados sujos como \"Fatal(2)\". Extraímos apenas a categoria textual (\"Fatal\").\n",
    "4.  **Tipagem:** Conversão explícita de colunas numéricas e booleanas para garantir integridade na inserção no banco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "8db4bb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando tratamento de tipos e valores nulos...\n",
      "   Aplicando limite de 50 caracteres...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATA\\AppData\\Local\\Temp\\ipykernel_7460\\3288826230.py:8: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Aplicando limite de 100 caracteres...\n",
      "   Aplicando limite de 200 caracteres...\n",
      "Tipagem e ajuste de tamanho concluídos.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. TRANSFORM: DATA CLEANING E TYPE CASTING (CORRIGIDO) ---\n",
    "print(\"Iniciando tratamento de tipos e valores nulos...\")\n",
    "\n",
    "# 1. Tratamento de Datas\n",
    "colunas_data = ['event_date', 'publication_date']\n",
    "for col in colunas_data:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True)\n",
    "\n",
    "# 2. Tratamento de Inteiros (Vítimas e Motores)\n",
    "colunas_numericas = [\n",
    "    'total_fatal_injuries', 'total_serious_injuries', \n",
    "    'total_minor_injuries', 'total_uninjured', 'number_of_engines'\n",
    "]\n",
    "for col in colunas_numericas:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# 3. Limpeza da coluna Severidade (Regra de Negócio)\n",
    "if 'injury_severity' in df.columns:\n",
    "    df['injury_severity'] = df['injury_severity'].astype(str).str.split('(').str[0].str.strip()\n",
    "\n",
    "# 4. Tratamento de Booleano (Amateur Built)\n",
    "if 'amateur_built' in df.columns:\n",
    "    df['amateur_built'] = df['amateur_built'].astype(str).str.lower().isin(['yes', 'y', 'true', '1'])\n",
    "\n",
    "# --- CORREÇÃO DO TRUNCATE (CORTE DE TEXTOS LONGOS) ---\n",
    "# A correção aqui remove o 'case=False' e usa uma lista explícita de valores para substituir\n",
    "\n",
    "# Grupo A: Colunas com limite de 50 caracteres (VARCHAR 50)\n",
    "cols_limit_50 = [\n",
    "    'event_id', 'investigation_type', 'accident_number', 'injury_severity',\n",
    "    'aircraft_damage', 'aircraft_category', 'registration_number', \n",
    "    'engine_type', 'schedule', 'weather_condition', 'report_status'\n",
    "]\n",
    "\n",
    "print(\"   Aplicando limite de 50 caracteres...\")\n",
    "for col in cols_limit_50:\n",
    "    if col in df.columns:\n",
    "        # 1. Converte para string e remove espaços\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "        # 2. Remove o texto 'nan' que aparece ao converter nulos para string\n",
    "        df[col] = df[col].replace(['nan', 'NaN', 'NAN', 'None'], '')\n",
    "        # 3. Corta no limite e garante que vazio vire None (para o banco entender como NULL)\n",
    "        df[col] = df[col].apply(lambda x: x[:50] if x else None)\n",
    "\n",
    "# Grupo B: Colunas com limite de 100 caracteres (VARCHAR 100)\n",
    "cols_limit_100 = ['make', 'model', 'country', 'purpose_of_flight', 'broad_phase_of_flight']\n",
    "\n",
    "print(\"   Aplicando limite de 100 caracteres...\")\n",
    "for col in cols_limit_100:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "        df[col] = df[col].replace(['nan', 'NaN', 'NAN', 'None'], '')\n",
    "        df[col] = df[col].apply(lambda x: x[:100] if x else None)\n",
    "\n",
    "# Grupo C: Colunas com limite de 200 caracteres (VARCHAR 200)\n",
    "cols_limit_200 = ['location', 'airport_name', 'air_carrier', 'far_description']\n",
    "\n",
    "print(\"   Aplicando limite de 200 caracteres...\")\n",
    "for col in cols_limit_200:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "        df[col] = df[col].replace(['nan', 'NaN', 'NAN', 'None'], '')\n",
    "        df[col] = df[col].apply(lambda x: x[:200] if x else None)\n",
    "        \n",
    "print(\"Tipagem e ajuste de tamanho concluídos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f594c4eb",
   "metadata": {},
   "source": [
    "## 5. Validação de Regras de Qualidade (Data Quality)\n",
    "\n",
    "Antes da carga, aplicamos filtros de consistência:\n",
    "* **Geolocalização:** Coordenadas com valores impossíveis (Latitude > 90 ou Longitude > 180) são convertidas para nulo, pois indicam erro de preenchimento.\n",
    "* **Unicidade:** O campo `event_id` é a chave primária da tabela. Removemos registros duplicados mantendo a primeira ocorrência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "51af0057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atenção: Foram removidos 938 registros duplicados.\n"
     ]
    }
   ],
   "source": [
    "# Função para validar limites geográficos\n",
    "def validar_coordenada(valor, limite_maximo):\n",
    "    try:\n",
    "        valor_float = float(valor)\n",
    "        if abs(valor_float) <= limite_maximo:\n",
    "            return valor_float\n",
    "        return None # Retorna nulo se estiver fora do limite\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Aplicação da validação\n",
    "df['latitude'] = df['latitude'].apply(lambda x: validar_coordenada(x, 90))\n",
    "df['longitude'] = df['longitude'].apply(lambda x: validar_coordenada(x, 180))\n",
    "\n",
    "# Remoção de duplicatas baseada no ID do evento\n",
    "qtd_inicial = len(df)\n",
    "df = df.drop_duplicates(subset=['event_id'], keep='first')\n",
    "qtd_removida = qtd_inicial - len(df)\n",
    "\n",
    "if qtd_removida > 0:\n",
    "    print(f\"Atenção: Foram removidos {qtd_removida} registros duplicados.\")\n",
    "else:\n",
    "    print(\"Nenhuma duplicidade de ID encontrada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c410c7c7",
   "metadata": {},
   "source": [
    "## 6. Carga no Data Warehouse (Load)\n",
    "\n",
    "Os dados tratados serão inseridos no esquema `silver` do PostgreSQL.\n",
    "Utilizamos a estratégia de **Truncate and Load** (limpar a tabela e inserir tudo novamente). Isso foi escolhido para garantir que, caso o script precise ser rodado várias vezes durante o desenvolvimento, não geremos dados duplicados no banco.\n",
    "\n",
    "Primeiramente, garantimos que a tabela existe executando o script DDL. Em seguida, realizamos a carga em lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "da66de5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando conexão com o Banco de Dados...\n",
      "DDL executado com sucesso (Tabela criada/atualizada).\n",
      "Tabela silver.aviao limpa.\n",
      "Inserindo dados...\n",
      "Processo finalizado. 87951 registros inseridos com sucesso.\n"
     ]
    }
   ],
   "source": [
    "print(\"Iniciando conexão com o Banco de Dados...\")\n",
    "\n",
    "try:\n",
    "    engine = create_engine(DB_URI)\n",
    "    \n",
    "    # Passo 1: Execução do DDL (Garantia de Estrutura)\n",
    "    if os.path.exists(ARQUIVO_DDL):\n",
    "        with open(ARQUIVO_DDL, 'r', encoding='utf-8') as f:\n",
    "            sql_ddl = f.read()\n",
    "            \n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(sql_ddl))\n",
    "            conn.commit()\n",
    "            print(\"DDL executado com sucesso (Tabela criada/atualizada).\")\n",
    "    \n",
    "    # Passo 2: Limpeza da Tabela (Truncate)\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"TRUNCATE TABLE silver.aviao CASCADE;\"))\n",
    "        conn.commit()\n",
    "        print(\"Tabela silver.aviao limpa.\")\n",
    "    \n",
    "    # Passo 3: Inserção dos Dados\n",
    "    print(\"Inserindo dados...\")\n",
    "    df.to_sql(\n",
    "        name='aviao',\n",
    "        con=engine,\n",
    "        schema='silver',\n",
    "        if_exists='append', # Adiciona aos dados existentes (que acabamos de limpar)\n",
    "        index=False,\n",
    "        chunksize=1000 # Insere em lotes de 1000 linhas para melhor performance\n",
    "    )\n",
    "    \n",
    "    print(f\"Processo finalizado. {len(df)} registros inseridos com sucesso.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro durante a carga no banco: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "e7b2e9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Relatório Final de Carga ---\n",
      "Total de linhas na tabela silver.aviao: 87951\n",
      "\n",
      "Amostra dos dados carregados:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>event_date</th>\n",
       "      <th>make</th>\n",
       "      <th>injury_severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001218X45444</td>\n",
       "      <td>1948-10-24</td>\n",
       "      <td>Stinson</td>\n",
       "      <td>Fatal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20001218X45447</td>\n",
       "      <td>1962-07-19</td>\n",
       "      <td>Piper</td>\n",
       "      <td>Fatal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20061025X01555</td>\n",
       "      <td>1974-08-30</td>\n",
       "      <td>Cessna</td>\n",
       "      <td>Fatal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20001218X45448</td>\n",
       "      <td>1977-06-19</td>\n",
       "      <td>Rockwell</td>\n",
       "      <td>Fatal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20041105X01764</td>\n",
       "      <td>1979-08-02</td>\n",
       "      <td>Cessna</td>\n",
       "      <td>Fatal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         event_id  event_date      make injury_severity\n",
       "0  20001218X45444  1948-10-24   Stinson           Fatal\n",
       "1  20001218X45447  1962-07-19     Piper           Fatal\n",
       "2  20061025X01555  1974-08-30    Cessna           Fatal\n",
       "3  20001218X45448  1977-06-19  Rockwell           Fatal\n",
       "4  20041105X01764  1979-08-02    Cessna           Fatal"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verificação simples dos dados inseridos\n",
    "print(\"\\n--- Relatório Final de Carga ---\")\n",
    "\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        # Contagem total\n",
    "        result = conn.execute(text(\"SELECT COUNT(*) FROM silver.aviao\"))\n",
    "        total = result.scalar()\n",
    "        print(f\"Total de linhas na tabela silver.aviao: {total}\")\n",
    "        \n",
    "        # Amostra de dados\n",
    "        print(\"\\nAmostra dos dados carregados:\")\n",
    "        df_amostra = pd.read_sql(\"SELECT event_id, event_date, make, injury_severity FROM silver.aviao LIMIT 5\", conn)\n",
    "        display(df_amostra)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(\"Não foi possível realizar a consulta de validação.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
